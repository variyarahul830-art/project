from fastapi import APIRouter, HTTPException, status
from config import settings
from schemas import ChatRequest
from services import hasura_client
from services.embeddings import EmbeddingGenerator
from services.milvus_service import MilvusService
from services.llm_service import LLMService
from services.redis_cache import get_redis_cache
from tasks.llm_tasks import generate_llm_answer_task
import logging
import json
import uuid
from datetime import datetime

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/chat", tags=["Chat"])


@router.post("/", response_model=dict)
async def chat(request: ChatRequest):
    """
    Send a question and get connected target nodes from the knowledge graph.
    IMPORTANT: Searches across ALL workflows regardless of workflow_id parameter.
    
    Priority:
    1. Exact match on source nodes (across all workflows) -> return their targets
    2. Partial/fuzzy match on source nodes (across all workflows) -> return their targets
    3. Fallback to RAG (PDF embeddings + LLM)
    
    - **question**: The user's question/source node text (required)
    - **workflow_id**: Ignored - always searches across all workflows
    """
    try:
        logger.info(f"Processing question: {request.question}")
        logger.info(f"Searching across ALL workflows (workflow_id parameter ignored)")
        
        # Resolve session_id if numeric (convert id -> session_id)
        actual_session_id = request.session_id
        if request.session_id and request.user_id:
            # Check if session_id is numeric (meaning it's the 'id' field)
            if request.session_id.isdigit():
                logger.info(f"Converting numeric session ID {request.session_id} to actual session_id")
                # Get all user sessions and find the one with this id
                sessions = await hasura_client.get_user_chat_sessions(request.user_id)
                session = next((s for s in sessions if str(s.get("id")) == request.session_id), None)
                if session:
                    actual_session_id = session.get("session_id")
                    logger.info(f"Resolved session_id: {actual_session_id}")
                else:
                    logger.warning(f"Session with id={request.session_id} not found for user={request.user_id}")
        
        # Initialize variables for saving to database
        answer_text = None
        source_type = None
        
        # Step 1: Try exact match on source node (search ALL workflows)
        logger.info("Step 1: Trying exact text match on source nodes across ALL workflows...")
        source_node = await hasura_client.get_node_by_text(request.question, workflow_id=None)
        
        if source_node:
            logger.info(f"Found exact source node: {source_node['text']}")
            target_nodes = await hasura_client.get_target_nodes(source_node["id"])
            
            if target_nodes:
                logger.info(f"Found {len(target_nodes)} target nodes from exact source match")
                clickable_answers = []
                for target_node in target_nodes:
                    further_nodes = await hasura_client.get_further_options(target_node["id"])
                    is_source = len(further_nodes) > 0 if further_nodes else False
                    clickable_answers.append({
                        "text": target_node["text"],
                        "id": target_node["id"],
                        "is_source": is_source
                    })
                
                logger.info(f"üìç Answer from NODE")
                response_data = {
                    "success": True,
                    "question": source_node["text"],
                    "answers": [node["text"] for node in target_nodes],
                    "target_nodes": [
                        {
                            "id": node["id"],
                            "text": node["text"],
                            "is_source": clickable_answers[[t["id"] for t in target_nodes].index(node["id"])]["is_source"],
                        }
                        for node in target_nodes
                    ],
                    "source": "knowledge_graph",
                    "data_source": "NODE",
                    "count": len(target_nodes)
                }
                
                # Save question and answer to chat history
                if actual_session_id and request.user_id:
                    try:
                        message_id = f"msg_{uuid.uuid4().hex[:16]}"
                        logger.info(f"üíæ Saving knowledge_graph message (session={actual_session_id}, user={request.user_id})")
                        await hasura_client.add_chat_message(
                            message_id=message_id,
                            session_id=actual_session_id,
                            user_id=request.user_id,
                            question=request.question,
                            answer=json.dumps(response_data),
                            source="knowledge_graph"
                        )
                        logger.info(f"‚úÖ Saved message {message_id}")
                    except Exception as e:
                        logger.error(f"‚ùå Failed to save chat message: {e}", exc_info=True)
                else:
                    logger.warning(f"‚ö†Ô∏è  Skipping save - session_id={actual_session_id}, user_id={request.user_id}")
                
                return response_data
        
        # Step 2: Try partial/fuzzy matching on all source nodes (search ALL workflows)
        logger.info("Step 2: Trying partial text match on source nodes across ALL workflows...")
        matching_nodes = await hasura_client.search_nodes_by_text(request.question, workflow_id=None)
        
        if matching_nodes:
            logger.info(f"Found {len(matching_nodes)} matching source nodes via partial search")
            # Get all target nodes for all matching nodes
            all_target_nodes = []
            for source in matching_nodes:
                targets = await hasura_client.get_target_nodes(source["id"])
                all_target_nodes.extend(targets)
            
            # Remove duplicates while preserving order
            seen = set()
            unique_targets = []
            for node in all_target_nodes:
                if node["id"] not in seen:
                    seen.add(node["id"])
                    unique_targets.append(node)
            
            if unique_targets:
                logger.info(f"Found {len(unique_targets)} unique target nodes from partial source matches")
                clickable_answers = []
                for target_node in unique_targets:
                    further_nodes = await hasura_client.get_further_options(target_node["id"])
                    is_source = len(further_nodes) > 0 if further_nodes else False
                    clickable_answers.append({
                        "text": target_node["text"],
                        "id": target_node["id"],
                        "is_source": is_source
                    })
                
                logger.info(f"üìç Answer from NODE")
                response_data = {
                    "success": True,
                    "question": request.question,
                    "answers": [node["text"] for node in unique_targets],
                    "target_nodes": [
                        {
                            "id": node["id"],
                            "text": node["text"],
                            "is_source": clickable_answers[[t["id"] for t in unique_targets].index(node["id"])]["is_source"],
                        }
                        for node in unique_targets
                    ],
                    "source": "knowledge_graph",
                    "data_source": "NODE",
                    "count": len(unique_targets)
                }
                
                # Save question and answer to chat history
                if actual_session_id and request.user_id:
                    try:
                        message_id = f"msg_{uuid.uuid4().hex[:16]}"
                        await hasura_client.add_chat_message(
                            message_id=message_id,
                            session_id=actual_session_id,
                            user_id=request.user_id,
                            question=request.question,
                            answer=json.dumps(response_data),
                            source="knowledge_graph"
                        )
                    except Exception as e:
                        logger.warning(f"Failed to save chat message: {e}")
                
                return response_data
            else:
                logger.info("Partial match found source nodes, but they have no target connections")
        
        # Step 2: Try to find answer in FAQs (with Redis caching)
        logger.info("Step 3: Checking Redis cache and FAQs...")
        
        # Initialize Redis cache
        redis_cache = get_redis_cache()
        redis_logger = logging.getLogger('redis')
        
        # Step 3a: Check Redis cache first
        cached_answer = redis_cache.get(request.question)
        if cached_answer:
            redis_logger.info(f"Cache hit - answer found")
            logger.info(f"‚ö° Answer from REDIS")
            response_data = {
                "success": True,
                "question": request.question,
                "answer": cached_answer.get("answer"),
                "source": cached_answer.get("source"),
                "faq_id": cached_answer.get("faq_id"),
                "category": cached_answer.get("category"),
                "from_cache": True,
                "data_source": "REDIS"
            }
            
            # Save question and answer to chat history
            if actual_session_id and request.user_id:
                try:
                    message_id = f"msg_{uuid.uuid4().hex[:16]}"
                    await hasura_client.add_chat_message(
                        message_id=message_id,
                        session_id=actual_session_id,
                        user_id=request.user_id,
                        question=request.question,
                        answer=cached_answer.get("answer"),
                        source="faq_cache"
                    )
                except Exception as e:
                    logger.warning(f"Failed to save cached chat message: {e}")
            
            return response_data
        
        # Step 3b: Search FAQs (exact match)
        redis_logger.info(f"Cache miss - searching FAQs...")
        logger.info("Cache miss - Searching FAQs for exact match...")
        faq_exact = await hasura_client.search_faq_exact(request.question)
        
        if faq_exact:
            logger.info(f"üìö Answer from FAQ")
            response_data = {
                "success": True,
                "question": request.question,
                "answer": faq_exact["answer"],
                "source": "faq",
                "faq_id": faq_exact["id"],
                "category": faq_exact.get("category"),
                "from_cache": False,
                "data_source": "FAQ"
            }
            
            # Cache this FAQ answer for future queries (20 min TTL)
            ttl = settings.REDIS_FAQ_TTL_MINUTES
            redis_logger.info(f"Caching FAQ answer (TTL: {ttl}min)")
            logger.info(f"Caching FAQ answer in Redis (TTL: {ttl}min)...")
            redis_cache.set(
                request.question,
                {
                    "answer": faq_exact["answer"],
                    "source": "faq",
                    "faq_id": faq_exact["id"],
                    "category": faq_exact.get("category")
                },
                ttl_minutes=ttl
            )
            
            # Save question and answer to chat history
            if actual_session_id and request.user_id:
                try:
                    message_id = f"msg_{uuid.uuid4().hex[:16]}"
                    await hasura_client.add_chat_message(
                        message_id=message_id,
                        session_id=actual_session_id,
                        user_id=request.user_id,
                        question=request.question,
                        answer=faq_exact["answer"],
                        source="faq"
                    )
                except Exception as e:
                    logger.warning(f"Failed to save chat message: {e}")
            
            return response_data
        
        # Step 3c: Try partial FAQ match
        logger.info("No exact match found - Searching for partial FAQ match...")
        faq_partial = await hasura_client.search_faq_partial(request.question)
        if faq_partial:
            logger.info(f"üìö Answer from FAQ")
            # Return the first (most relevant) partial match
            best_faq = faq_partial[0]
            response_data = {
                "success": True,
                "question": request.question,
                "answer": best_faq["answer"],
                "source": "faq",
                "faq_id": best_faq["id"],
                "category": best_faq.get("category"),
                "match_type": "partial",
                "from_cache": False,
                "data_source": "FAQ"
            }
            
            # ‚ú® NEW: Cache this partial FAQ answer for future queries (20 min TTL)
            ttl = settings.REDIS_FAQ_TTL_MINUTES
            logger.info(f"üíæ Caching partial FAQ answer in Redis (TTL: {ttl}min)...")
            redis_cache.set(
                request.question,
                {
                    "answer": best_faq["answer"],
                    "source": "faq",
                    "faq_id": best_faq["id"],
                    "category": best_faq.get("category"),
                    "match_type": "partial"
                },
                ttl_minutes=ttl
            )
            
            # Save question and answer to chat history
            if actual_session_id and request.user_id:
                try:
                    message_id = f"msg_{uuid.uuid4().hex[:16]}"
                    await hasura_client.add_chat_message(
                        message_id=message_id,
                        session_id=actual_session_id,
                        user_id=request.user_id,
                        question=request.question,
                        answer=best_faq["answer"],
                        source="faq"
                    )
                except Exception as e:
                    logger.warning(f"Failed to save chat message: {e}")
            
            return response_data
        
        logger.info("No FAQs found, trying RAG approach with PDF embeddings")
        
        # Step 2: If not found in knowledge graph, use RAG approach
        # Generate embedding for the question
        logger.info("Generating embedding for question...")
        embedding_generator = EmbeddingGenerator(
            model_name=settings.EMBEDDING_MODEL,
            huggingface_token=settings.HUGGINGFACE_TOKEN if settings.HUGGINGFACE_TOKEN else None
        )
        question_embedding = embedding_generator.generate_embedding(request.question)
        logger.info(f"‚úÖ Embedding generated (dimension: {len(question_embedding)})")
        
        # Search for similar chunks in Milvus
        logger.info("Searching for similar chunks in Milvus...")
        milvus_service = MilvusService(
            host=settings.MILVUS_HOST,
            port=settings.MILVUS_PORT,
            collection_name=settings.MILVUS_COLLECTION_NAME
        )
        
        try:
            # Create collection if it doesn't exist
            milvus_service.create_collection(embedding_dim=len(question_embedding))
        except Exception as e:
            logger.warning(f"Collection creation/verification had an issue: {str(e)}")
        
        # Search for top 5 similar chunks
        similar_chunks = milvus_service.search_embeddings(question_embedding, limit=5)
        
        if not similar_chunks:
            logger.warning("No similar chunks found in Milvus")
            return {
                "success": False,
                "question": request.question,
                "answer": None,
                "source": "rag",
                "message": "No relevant information found in documents. Please try another question or upload documents.",
                "chunks_found": 0
            }
        
        logger.info(f"‚úÖ Found {len(similar_chunks)} similar chunks")
        
        # Submit async Celery task for LLM answer generation via RabbitMQ
        logger.info("üì§ Submitting LLM answer generation task to RabbitMQ (Celery)...")
        
        message_id = f"msg_{uuid.uuid4().hex[:16]}"
        rabbitmq_logger = logging.getLogger('rabbitmq')
        
        try:
            # Submit task to RabbitMQ queue
            rabbitmq_logger.info(f"Task submitted: {message_id}")
            celery_task = generate_llm_answer_task.apply_async(
                args=[
                    request.question,
                    similar_chunks,
                    actual_session_id,
                    request.user_id,
                    message_id
                ],
                queue='llm_tasks',
                task_id=message_id
            )
            
            logger.info(f"Task submitted with ID: {celery_task.id}")
            
            # Wait for task completion with timeout (up to 60 seconds)
            logger.info(f"Waiting for LLM task to complete (timeout: 60s)...")
            
            try:
                result = celery_task.get(timeout=60)
                rabbitmq_logger.info(f"Task completed: {celery_task.id}")
                logger.info(f"Task completed: {result}")
                
                response_data = result.get('response_data', {})
                
                # Ensure we have a proper response
                if not response_data.get('success'):
                    response_data = {
                        "success": True,
                        "question": request.question,
                        "answer": result.get('answer'),
                        "source": "rag",
                        "data_source": "RAG",
                        "chunks_used": len(similar_chunks),
                        "source_documents": [
                            {
                                "document": chunk.get("document_name"),
                                "page": chunk.get("page_number"),
                                "relevance_score": round(chunk.get("score", 0), 4)
                            } for chunk in similar_chunks
                        ]
                    }
                
                logger.info(f"ü§ñ Answer from RAG (via RabbitMQ/Celery)")
                return response_data
                
            except Exception as timeout_exc:
                # Task is still processing - return task ID for client polling
                rabbitmq_logger.warning(f"‚ö†Ô∏è  TASK TIMEOUT:")
                rabbitmq_logger.warning(f"   Task ID: {celery_task.id}")
                rabbitmq_logger.warning(f"   Timeout: 60 seconds exceeded")
                rabbitmq_logger.warning(f"   Task processing in background...")
                logger.warning(f"‚ö†Ô∏è  Task timeout or error: {str(timeout_exc)}")
                logger.info(f"üìã Returning task ID for client polling: {celery_task.id}")
                
                return {
                    "success": True,
                    "question": request.question,
                    "answer": None,
                    "source": "rag",
                    "data_source": "RAG_ASYNC",
                    "task_id": celery_task.id,
                    "status": "processing",
                    "message": "Answer is being generated asynchronously. Use task_id to poll for results.",
                    "chunks_used": len(similar_chunks)
                }
        
        except Exception as task_exc:
            logger.error(f"‚ùå Failed to submit Celery task: {str(task_exc)}", exc_info=True)
            # Fallback to synchronous generation if task submission fails
            logger.info("‚ö†Ô∏è  Falling back to synchronous LLM generation...")
            
            # Get token from settings
            hf_token = settings.HUGGINGFACE_TOKEN if hasattr(settings, 'HUGGINGFACE_TOKEN') else None
            if not hf_token:
                logger.error("HUGGINGFACE_TOKEN not configured in settings")
                return {
                    "success": False,
                    "question": request.question,
                    "answer": None,
                    "source": "rag",
                    "message": "LLM service not properly configured. Please set HUGGINGFACE_TOKEN.",
                    "chunks_found": len(similar_chunks)
                }
            
            llm_service = LLMService(
                huggingface_token=hf_token,
                model_name=settings.LLM_MODEL if hasattr(settings, 'LLM_MODEL') else "meta-llama/Llama-2-7b-chat-hf"
            )
            answer = llm_service.generate_answer_with_context(
                question=request.question,
                context_chunks=similar_chunks,
                max_length=512,
                temperature=0.7
            )
            
            logger.info("‚úÖ Answer generated successfully (synchronous fallback)")
            
            # Get unique PDFs with their IDs for download links
            pdf_doc_map = {}
            for chunk in similar_chunks:
                doc_name = chunk.get("document_name")
                if doc_name and doc_name not in pdf_doc_map:
                    # Get PDF ID from database
                    pdf_id = await hasura_client.get_pdf_id_by_name(doc_name)
                    if pdf_id:
                        pdf_doc_map[doc_name] = pdf_id
            
            # Return answer with source information
            logger.info("ü§ñ Answer from RAG (synchronous fallback)")
            response_data = {
                "success": True,
                "question": request.question,
                "answer": answer,
                "source": "rag",
                "data_source": "RAG_SYNC",
                "chunks_used": len(similar_chunks),
                "source_documents": [
                    {
                        "document": chunk.get("document_name"),
                        "page": chunk.get("page_number"),
                        "relevance_score": round(chunk.get("score", 0), 4),
                        "pdf_id": pdf_doc_map.get(chunk.get("document_name"))
                    } for chunk in similar_chunks
                ]
            }
            
            # Save question and answer to chat history
            if actual_session_id and request.user_id:
                try:
                    message_id = f"msg_{uuid.uuid4().hex[:16]}"
                    logger.info(f"üíæ Saving RAG message (session={actual_session_id}, user={request.user_id})")
                    await hasura_client.add_chat_message(
                        message_id=message_id,
                        session_id=actual_session_id,
                        user_id=request.user_id,
                        question=request.question,
                        answer=answer,
                        source="rag"
                    )
                    logger.info(f"‚úÖ RAG message saved")
                except Exception as e:
                    logger.error(f"‚ùå Failed to save RAG chat message: {e}", exc_info=True)
            else:
                logger.warning(f"‚ö†Ô∏è  Skipping RAG save - session_id={actual_session_id}, user_id={request.user_id}")
            
            return response_data
        
    
    except Exception as e:
        logger.error(f"‚ùå Error processing question: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to process question: {str(e)}"
        )


@router.get("/cache/info", response_model=dict)
async def get_cache_info():
    """
    Get Redis cache statistics and information (Requires Authentication)
    
    Returns:
    - connected: Whether Redis is connected
    - faq_cached_items: Number of cached FAQ answers
    - redis_memory_used: Memory usage by Redis
    - redis_version: Redis server version
    - uptime_seconds: Redis server uptime
    """
    try:
        redis_cache = get_redis_cache()
        cache_info = redis_cache.get_cache_info()
        
        if cache_info:
            logger.info(f"Cache info retrieved: {cache_info['faq_cached_items']} items cached")
            return {
                "success": True,
                "cache_info": cache_info
            }
        else:
            logger.warning("Redis cache not available")
            return {
                "success": False,
                "cache_info": {
                    "connected": False,
                    "message": "Redis cache not available"
                }
            }
    except Exception as e:
        logger.error(f"‚ùå Error getting cache info: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get cache info: {str(e)}"
        )


@router.delete("/cache/clear", response_model=dict)
async def clear_faq_cache():
    """
    Clear all FAQ cache entries (use with caution - Requires Authentication)
    
    Returns:
    - success: Whether cache was cleared
    - message: Status message
    """
    try:
        redis_cache = get_redis_cache()
        result = redis_cache.clear_all()
        
        if result:
            logger.info("‚úÖ FAQ cache cleared successfully")
            return {
                "success": True,
                "message": "All FAQ cache entries cleared successfully"
            }
        else:
            logger.warning("Redis cache not available for clearing")
            return {
                "success": False,
                "message": "Redis cache not available"
            }
    except Exception as e:
        logger.error(f"‚ùå Error clearing cache: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to clear cache: {str(e)}"
        )


@router.get("/task/{task_id}", response_model=dict)
async def get_task_result(task_id: str):
    """
    Poll for Celery task result (for async RAG answer generation)
    
    Args:
        task_id (str): The Celery task ID returned from the chat endpoint
    
    Returns:
        dict: Task status and result if available
    """
    try:
        from celery.result import AsyncResult
        from celery_config import app as celery_app
        
        logger.info(f"üìã Polling task result for task_id: {task_id}")
        
        # Get task result from Celery
        task_result = AsyncResult(task_id, app=celery_app)
        
        if task_result.state == 'PENDING':
            logger.info(f"‚è≥ Task {task_id} is still pending")
            return {
                "success": True,
                "task_id": task_id,
                "status": "pending",
                "message": "Task is still being processed"
            }
        
        elif task_result.state == 'SUCCESS':
            logger.info(f"‚úÖ Task {task_id} completed successfully")
            result = task_result.result
            return {
                "success": True,
                "task_id": task_id,
                "status": "success",
                "result": result,
                "response_data": result.get('response_data') if isinstance(result, dict) else None
            }
        
        elif task_result.state == 'FAILURE':
            logger.error(f"‚ùå Task {task_id} failed")
            return {
                "success": False,
                "task_id": task_id,
                "status": "failure",
                "error": str(task_result.info),
                "message": "Task failed to complete"
            }
        
        elif task_result.state == 'RETRY':
            logger.warning(f"üîÑ Task {task_id} is retrying")
            return {
                "success": True,
                "task_id": task_id,
                "status": "retry",
                "message": "Task is retrying after a failure"
            }
        
        else:
            logger.info(f"‚ÑπÔ∏è  Task {task_id} state: {task_result.state}")
            return {
                "success": True,
                "task_id": task_id,
                "status": task_result.state.lower(),
                "message": f"Task is in {task_result.state} state"
            }
    
    except Exception as e:
        logger.error(f"‚ùå Error polling task result: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get task result: {str(e)}"
        )

